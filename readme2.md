### Notes on Prompt Engineering and In-Context Learning

#### Key Terminology
- **Prompt**: The text input provided to the model.
- **Inference**: The act of generating text from the model.
- **Completion**: The output text generated by the model.
- **Context Window**: The total amount of text (memory) available for the prompt.

#### Understanding Prompt Engineering
- **Prompt Engineering**: The process of developing and refining prompts to guide the model's behavior.
  - It involves revising the language or structure of the prompt to achieve the desired outcome.

#### Strategies for Effective Prompt Engineering
1. **In-Context Learning**:
   - Including examples within the prompt to help the model understand the task better.
   - Examples are included in the context window.

2. **Zero-Shot Inference**:
   - Providing the task without any examples.
   - Suitable for large models that can grasp tasks without examples.
   - Example: "Classify this review" followed by the review text.

3. **One-Shot Inference**:
   - Including one example within the prompt to demonstrate the task.
   - Helps smaller models understand the task.
   - Example: Providing one example of a review and its sentiment before asking the model to classify a new review.

4. **Few-Shot Inference**:
   - Including multiple examples within the prompt.
   - Useful when one example is not sufficient for the model to understand the task.
   - Example: Providing both positive and negative review examples to guide the model.

#### Model Size and Performance
- **Large Models**:
  - Typically good at zero-shot inference.
  - Can perform a wide range of tasks without needing specific training.
  - More parameters lead to better language understanding and task performance.

- **Smaller Models**:
  - Often require one-shot or few-shot inference.
  - Better suited for tasks similar to those they were trained on.
  - May need multiple examples to perform well on a new task.

#### Fine-Tuning
- **Fine-Tuning**:
  - Additional training on the model using new data specific to the desired task.
  - An alternative when in-context learning with multiple examples is insufficient.
  - Will be explored in detail in week 2 of the course.

#### Practical Tips
- **Context Window Limitations**:
  - Be mindful of the context window limit when adding examples.
  - If the modelâ€™s performance doesn't improve after several examples, consider fine-tuning.

- **Model Selection**:
  - Experiment with different models to find the best fit for your use case.
  - Adjust settings to influence the structure and style of the completions.
